# -*- coding: utf-8 -*-
"""ML_part_(project)_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16926LUIByt9_GcgLo9ne80h79DsQ9z9V

# Read in data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')  #style sheet for the plots
import nltk

df = pd.read_excel('/content/amazondataset.xlsx')

#df = df.assign(id=range(1, len(df) + 1))

df.drop(columns=['countryCode','reviewImages/0','reviewImages/1','reviewImages/2','reviewImages/3',
                 'reviewImages/4','reviewImages/5','reviewImages/6','reviewImages/7','reviewImages/8',
                 'reviewImages/9','reviewImages/10','reviewImages/11','reviewImages/12','reviewUrl',
                 'reviewedIn','productAsin'], axis =1,
                 level=None, inplace=True)

df.isnull().sum(axis=0)

df=df.dropna(subset=['reviewDescription']).reset_index(drop=True)

df.head()

#merging reviews descriptions with their title
df["reviews"] = df['reviewTitle'] +"-"+ df["reviewDescription"]

df = df.drop(['reviewDescription','reviewTitle'],axis = 1)

df['isVerified'].value_counts() #188 commentaires non verifi√©s

"""# Basic NLTK + Vader (sentiment scoring) for dataset labeling"""

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
nltk.download('punkt')
from tqdm.notebook import tqdm

sia = SentimentIntensityAnalyzer()

df = df.astype({'reviews':'string'})

df.dtypes

nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')  #Chunking is defined as the process of natural language processing used to identify parts of speech and short phrases present in a given sentence.
nltk.download('words')
encoded = []
for str in df['reviews'] :
  tokens = nltk.word_tokenize(str)
  tagged = nltk.pos_tag(tokens)
  entities = nltk.chunk.ne_chunk(tagged)
  encoded.append(entities)
df['tokinized_reviews'] = encoded
#df.drop('reviewDescription',axis = 1,level=None, inplace=True)

# Run the polarity score on the entire dataset
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['reviews']
    id = row['position']  #reviewnum
    res[id] = sia.polarity_scores(text)

res[1455] #negative, positive score, neutral... and the compound score

"""store our res dictionary into a pandas dataframe named " df_vaders "
"""

df_vaders = pd.DataFrame(res).T
df_vaders = df_vaders.reset_index().rename(columns={'index': 'position'})
df_vaders = df_vaders.merge(df, how='left')

# Now we have sentiment score and metadata
df_vaders

"""Plot vader results"""

ax = sns.barplot(data=df_vaders,x='ratingScore', y='compound')
ax.set_title('Compound Score by Amazon Star Review')
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(12, 3))
sns.barplot(data=df_vaders, x='ratingScore', y='pos', ax=axs[0])
sns.barplot(data=df_vaders, x='ratingScore', y='neu', ax=axs[1])
sns.barplot(data=df_vaders, x='ratingScore', y='neg', ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()

df_vaders.query('compound < 0').count()

"""Adding a column named "sentiment" as labels for the reviews data"""

score = df_vaders["compound"].values
sentiment = []
for i in score:
    if i >= 0.05 :
        sentiment.append('Positive')
    elif i <= -0.05 :
        sentiment.append('Negative')
    else:
        sentiment.append('Neutral')
df_vaders["sentiment"] = sentiment
df_vaders.head()

df_vaders['sentiment'].value_counts()

df_vaders['ratingScore'].value_counts()

X = df_vaders.reviews.values
y = df_vaders.sentiment.replace(4,1)

"""#Natural language processing (NLP) processing

**Text cleaning**  X
"""

!pip install autocorrect

import re
from autocorrect import Speller
spell = Speller(lang='en')
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
lemm = WordNetLemmatizer()
import os

#Fixing Word Lengthening
def reduce_lengthening(text):
    pattern = re.compile(r"(.)\1{2,}")
    return pattern.sub(r"\1\1", text)

import nltk
nltk.download('omw-1.4')

def text_preprocess(doc):
    #Lowercasing all the letters
    temp = doc.lower()
    #Removing hashtags and mentions
    temp = re.sub("@[A-Za-z0-9_]+","", temp)
    temp = re.sub("#[A-Za-z0-9_]+","", temp)
    #Removing links
    temp = re.sub(r"http\S+", "", temp)
    temp = re.sub(r"www.\S+", "", temp)
    #removing numbers
    temp = re.sub("[0-9]","", temp)
    #Removing '
    temp = re.sub("'"," ",temp)

    #Tokenization
    temp = word_tokenize(temp)
    #Fixing Word Lengthening
    temp = [reduce_lengthening(w) for w in temp]
    #spell corrector
    temp = [spell(w) for w in temp]
    #stem
    temp = [lemm.lemmatize(w) for w in temp]
    #Removing short words                         
    temp = [w for w in temp if len(w)>2]  #
    temp = " ".join(w for w in temp)
    #temp = listToString(temp)
    
    return temp

df_vaders['clean_reviews'] = df_vaders['reviews'].apply(lambda x : text_preprocess(x))

df_vaders['clean_reviews'][0]

"""**Stemming** | finding the root of words. & **Lemmatization** | finding the from of words"""

from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
ps = PorterStemmer()
def stem_lem(x) :
  res = []
  tokens = word_tokenize(x)
  for word in tokens :
    lmt = lemmatizer.lemmatize(word)
    stm = ps.stem(lmt)
    res.append(stm)
  return res

df_vaders['clean_reviews'] = df_vaders['clean_reviews'].apply(lambda x : stem_lem(x))

df_vaders.head()

"""**Word embedding | converting words into numbers**"""

#convert list elements to string
def listToString(s):
    str1 = ""
    for ele in s:
        str1 = str1 + ele + ' '
    return str1

#convert list elements to string
def listToString2(l):
    return l[0]

def listToString3(s):
    str1 = ''
    for ele in s:
        str1 = str1 + str(ele) 
    return str1

df_vaders["clean_reviews"] = df_vaders["clean_reviews"].apply(listToString)

df_vaders['clean_reviews']

"""Vectorizing..."""

from sklearn.feature_extraction.text import TfidfVectorizer
def transform_string(string):
  # Create a TfidfVectorizer object
  vectorizer = TfidfVectorizer()
  # Transform the string into a numerical representation
  transformed = vectorizer.fit_transform([string])
  # Return the transformed string as a NumPy array
  return transformed.toarray()

df_vaders["vectorized_reviews"] = df_vaders["clean_reviews"].apply(transform_string)

df_vaders["vectorized_reviews2"] = df_vaders["vectorized_reviews"].apply(listToString2)

"""#*Encoding target data (sentiment) + data splitting + text vectorization*"""

def sentiment_to_label(sentiment):
    if sentiment == "Negative":
        return 0
    elif sentiment == "Neutral":
        return 1
    elif sentiment == "Positive":
        return 2

# Assume that your dataframe is called "df" and the column containing the sentiments is called "sentiment"
df_vaders["labels"] = df_vaders["sentiment"].apply(sentiment_to_label)

df_vaders.head()

from sklearn.model_selection import train_test_split
y = df_vaders["labels"]
X = df_vaders["clean_reviews"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""...."""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(binary = True)
cv.fit(X_train)
cv.fit(X_test)
X_train = cv.transform(X_train)
X_test = cv.transform(X_test)

"""#Applying and evaluating some classification models

logistic regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

for c in [0.01 , 0.05, 0.25, 0.5, 1] :
  lr = LogisticRegression(C=c)
  lr.fit(X_train,y_train)
  print("Accuracy for C = %s: %s"% (c, accuracy_score(y_test,lr.predict(X_test))))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative','Neutral']
print(classification_report(y_test, lr.predict(X_test), target_names=target_names))

"""Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
model2 = MultinomialNB()
model2.fit(X_train,y_train)
print("Accuracy =  %s"% ( accuracy_score(y_test,model2.predict(X_test))))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative','Neutral']
print(classification_report(y_test, model2.predict(X_test), target_names=target_names))

"""SVM"""

from sklearn import svm
model3 = svm.SVC()
model3.fit(X_train,y_train)
print("Accuracy =  %s"% ( accuracy_score(y_test,model3.predict(X_test))))

"""....."""

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative','Neutral']
print(classification_report(y_test, model3.predict(X_test), target_names=target_names))